import logging
import random
import re

from aiocqhttp import CQHttp

from astrbot.api.all import *
from astrbot.api.event.filter import *

logger = logging.getLogger("astrbot")

@register("astrbot_plugin_anti_porn", "buding", "ä¸€ä¸ªç”¨äºåç‘Ÿç‘Ÿçš„æ’ä»¶", "1.0.0", "https://github.com/zouyonghe/astrbot_plugin_anti_porn")
class AntiPorn(Star):
    def __init__(self, context: Context, config: dict):
        super().__init__(context)
        self.config = config

    async def _admin_check(self, event: AstrMessageEvent, client: CQHttp) -> bool:
        """æ£€æŸ¥å½“å‰ bot æ˜¯å¦æ˜¯ç¾¤ç®¡ç†å‘˜æˆ–ç¾¤ä¸»å¹¶ä¸”æ¶ˆæ¯å‘é€è€…ä¸æ˜¯ç®¡ç†å‘˜æˆ–ç¾¤ä¸»"""
        try:
            sender_id = int(event.get_sender_id())
            group_id = int(event.get_group_id())
            bot_id = int(event.get_self_id())

            bot_info = await client.get_group_member_info(group_id=group_id, user_id=bot_id, no_cache=True, self_id=int(event.get_self_id()))
            sender_info = await client.get_group_member_info(group_id=group_id, user_id=sender_id, no_cache=True, self_id=int(event.get_self_id()))
            return bot_info.get("role") in ["admin", "owner"] and sender_info.get("role") not in ["admin", "owner"]
        except Exception as e:
            logging.error(f"è·å–ç¾¤æˆå‘˜ä¿¡æ¯å¤±è´¥: {e}")
            return False

    def _in_group_white_list(self, event: AstrMessageEvent) -> bool:
        group_white_list = self.config.get("group_white_list", "").split(";")
        if str(event.get_group_id()) in group_white_list:
            logger.debug(f"ç¾¤ {event.get_group_id()} åœ¨ç™½åå•å†…ï¼Œè·³è¿‡å®¡æŸ¥")
            return True
        return False

    async def _delete_and_ban(self, event: AstrMessageEvent, message: str, client: CQHttp):
        """åˆ é™¤æ¶ˆæ¯å¹¶ç¦è¨€ç”¨æˆ·"""
        try:
            await client.delete_msg(
                message_id=int(event.message_obj.message_id),
                self_id=int(event.get_self_id())
            )
            logger.info(f"Anti porn deleted message: {message}")

            await client.set_group_ban(
                group_id=int(event.get_group_id()),
                user_id=int(event.get_sender_id()),
                duration=self.config.get("group_ban_time", 5) * 60,
                self_id=int(event.get_self_id())
            )
            logger.info(f"Banned user: {event.get_sender_id()} for 5 minutes")

        except Exception as e:
            logger.error(f"Failed to delete and ban: {e}")

        event.stop_event()

    def _local_censor_check(self, message: str) -> bool:
        local_censor_keywords = self.config.get("local_censor_keywords", "").split(";")

        # å°†æ¶ˆæ¯è½¬æ¢ä¸ºå°å†™ï¼Œé¿å…å¤§å°å†™å¹²æ‰°
        message = message.lower()

        # å»é™¤æ¶ˆæ¯ä¸­çš„æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
        # åœ¨ä¸­æ–‡ä¸­ï¼Œæ ‡ç‚¹ç¬¦å·éœ€è¦ç‰¹åˆ«å¤„ç†
        message = re.sub(r"[^\w\u4e00-\u9fa5\s]", "", message)  # ä¿ç•™ä¸­æ–‡å­—ç¬¦å’Œå­—æ¯æ•°å­—

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•æ„Ÿè¯
        for keyword in local_censor_keywords:
            # å¦‚æœæ•æ„Ÿè¯åœ¨æ¶ˆæ¯ä¸­ä»¥å®Œæ•´è¯çš„å½¢å¼å‡ºç°ï¼Œåˆ™è®¤ä¸ºåŒ…å«è¯¥æ•æ„Ÿè¯
            # åœ¨ä¸­æ–‡ä¸­ï¼Œæ•æ„Ÿè¯å¯èƒ½æ˜¯è¿ç»­å­—ç¬¦ï¼Œå› æ­¤è¿™é‡Œä¸éœ€è¦ç©ºæ ¼åˆ†éš”
            if re.search(r"\b" + re.escape(keyword.lower()) + r"\b", message):
                return True
        return False

    async def _llm_censor_check(self, event: AstrMessageEvent, message: str) -> bool:
        """è°ƒç”¨ LLM è¿›è¡Œæ•æ„Ÿå†…å®¹æ£€æµ‹ï¼Œåªæœ‰åœ¨æ¶ˆæ¯å­—æ•° < 50 å¹¶ä¸”æ»¡è¶³æ¦‚ç‡è¦æ±‚æ—¶æ‰æ‰§è¡Œ"""
        llm_probability = float(self.config.get("llm_censor_probability", 0.1))
        if len(message) > 50 or random.random() > llm_probability:
            return False

        """è°ƒç”¨ LLM è¿›è¡Œæ•æ„Ÿå†…å®¹æ£€æµ‹"""
        provider = self.context.get_using_provider()
        if not provider:
            logger.warning("No available LLM provider")
            return False

        custom_guidelines = self.config.get("custom_guideline", "")
        censor_prompt = (
            f"Analyze the following message and determine whether it contains actual pornography or inappropriate content, "
            f"considering the overall context. Do not judge based on isolated words or phrases. "
            f"Additionally, consider the following user-defined guidelines when making your judgment:\n\n"
            f"{custom_guidelines}\n\n"
            f"Answer only 'Yes' or 'No' with no additional explanation.\n\n"
            f"Message: {message}"
        )

        try:
            response = await provider.text_chat(censor_prompt, session_id=str(event.get_sender_id()))
            if response and response.completion_text:
                result = response.completion_text.strip().lower()
                return result.startswith("yes") or result.endswith("yes")

        except Exception as e:
            logger.error(f"LLM censor request failed: {e}")

        return False

    @event_message_type(EventMessageType.GROUP_MESSAGE, priority=10)
    async def sensor_porn(self, event: AstrMessageEvent):
        """æ£€æµ‹æ¶ˆæ¯æ˜¯å¦åŒ…å«æ•æ„Ÿå†…å®¹"""
        if not self.config.get("enable_anti_porn", False):
            return

        if not self._in_group_white_list(event):
            return

        from astrbot.core.platform.sources.aiocqhttp.aiocqhttp_message_event import AiocqhttpMessageEvent
        assert isinstance(event, AiocqhttpMessageEvent)
        client = event.bot
        # æ£€æŸ¥botä¸ºç®¡ç†å‘˜ï¼Œæ¶ˆæ¯å‘é€è€…ä¸ä¸ºç®¡ç†å‘˜
        if not await self._admin_check(event, client):
            logging.debug("Bot ä¸æ˜¯è¯¥ç¾¤ç®¡ç†å‘˜ï¼Œæ— éœ€æ£€æµ‹ç¾¤èŠæ˜¯å¦åˆè§„")
            return

        for comp in event.get_messages():
            if isinstance(comp, BaseMessageComponent):
                message_content = comp.toString()
                logger.debug(f"Text message content: {message_content}")
                # æœ¬åœ°æ£€æŸ¥
                if self._local_censor_check(message_content):
                    logger.debug(f"Local sensor found illegal message: {message_content}")
                    await self._delete_and_ban(event, message_content, client)
                    return

                # è°ƒç”¨LLMæ£€æµ‹
                if await self._llm_censor_check(event, message_content):
                    logger.debug(f"LLM censor found illegal message: {message_content}")
                    await self._delete_and_ban(event, message_content, client)
                    return

    @permission_type(PermissionType.ADMIN)
    @command("anti_porn")
    async def anti_porn(self, event: AstrMessageEvent):
        """åˆ‡æ¢åç‘Ÿç‘Ÿæ¨¡å¼ï¼ˆenable_anti_pornï¼‰"""
        try:
            # è¯»å–å½“å‰çŠ¶æ€å¹¶å–å
            current_set = bool(self.config.get("enable_anti_porn", False))
            new_set = not current_set

            # æ›´æ–°é…ç½®
            self.config["enable_anti_porn"] = new_set

            # å‘é€åé¦ˆæ¶ˆæ¯
            status = "å¼€å¯" if new_set else "å…³é—­"
            yield event.plain_result(f"ğŸ“¢ åç‘Ÿç‘Ÿæ¨¡å¼å·²{status}")
        except Exception as e:
            logger.error(f"åˆ‡æ¢åç‘Ÿç‘Ÿæ¨¡å¼å¤±è´¥: {e}")
            yield event.plain_result("âŒ åˆ‡æ¢åç‘Ÿç‘Ÿæ¨¡å¼å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
